import streamlit as st
import pandas as pd

def main():
  st.title("å°ç£å—å³¶èª-è¯èªå¥åº«è³‡æ–™é›†")
  st.subheader("Dataset of Formosan-Mandarin sentence pairs")
  st.markdown(
        """
![visitors](https://visitor-badge.glitch.me/badge?page_id=howard-haowen.Formosan-languages)

### åŸå§‹ç¢¼
[GitHubé é¢](https://github.com/howard-haowen/Formosan-languages)

### è³‡æ–™é›†
- ğŸ¢ è³‡æ–™é›†åˆè¨ˆ`139102`ç­†å¥å°
- â€¼ï¸ æ­¤æŸ¥è©¢ç³»çµ±åƒ…ä¾›æ•™å­¸èˆ‡ç ”ç©¶ä¹‹ç”¨ï¼Œå…§å®¹ç‰ˆæ¬Šæ­¸åŸå§‹è³‡æ–™æä¾›è€…æ‰€æœ‰

### è³‡æ–™ä¾†æº
- ğŸ¥… ä¹éšæ•™æ: [æ—èªEæ¨‚åœ’](http://web.klokah.tw)
- ğŸ’¬ ç”Ÿæ´»æœƒè©±: [æ—èªEæ¨‚åœ’](http://web.klokah.tw)
- ğŸ§— å¥å‹: [æ—èªEæ¨‚åœ’](http://web.klokah.tw)
- ğŸ”­ æ–‡æ³•: [è‡ºç£å—å³¶èªè¨€å¢æ›¸](https://alilin.apc.gov.tw/tw/)
   + ä»¥ä¸Šä¾†æºçš„è³‡æ–™æ˜¯é€éç¶²è·¯çˆ¬èŸ²å–å¾—ã€‚
- ğŸ“š è©å…¸: [åŸä½æ°‘æ—èªè¨€ç·šä¸Šè©å…¸](https://e-dictionary.apc.gov.tw/Index.htm?fbclid=IwAR18XBJPj2xs7nhpPlIUZ-P3joQRGXx22rbVcUvp14ysQu6SdrWYvo7gWCc)
   + è©å…¸è³‡æ–™æ˜¯é€é`PDFMiner` å°‡2019ç‰ˆçš„PDFæª”è½‰æˆHTMLï¼Œå†ç”¨`BeautifulSoup`æŠ“å–å¥å°ï¼Œå¶çˆ¾æœƒå‡ºç¾æ—èªè·Ÿè¯èªå°ä¸ä¸Šçš„æƒ…å½¢ã€‚è‹¥ç™¼ç¾éŒ¯èª¤ï¼Œè«‹[è¯çµ¡æˆ‘ğŸ“©](https://github.com/howard-haowen)ã€‚è©å…¸ä¸­é‡è¤‡å‡ºç¾çš„å¥å­å·²å¾è³‡æ–™é›†ä¸­åˆªé™¤ã€‚

### ä½¿ç”¨æ–¹æ³•
- ğŸ”­ éæ¿¾ï¼šé¸æ“‡ä¾†æºèˆ‡èªè¨€
- ğŸ“š æ’åºï¼šé»é¸é¦–æ¬„
- ğŸ¥… æ”¾å¤§ï¼šé»é¸è¡¨æ ¼å³ä¸Šè§’é€²å…¥å…¨è¢å¹•æ¨¡å¼
- ğŸ’¬ æ›´å¤šï¼šå¥å­å¤ªé•·æ™‚ï¼Œå°‡æ»‘é¼ ç§»åˆ°å¥å­ä¸Šæ–¹å³å¯æª¢è¦–å®Œæ•´å…§å®¹
"""
)
  
  df = get_data()
  pd.set_option('max_colwidth', 600)
  sources = st.radio(
        "è«‹é¸æ“‡ä¾†æº",
        options=['è©å…¸', 'æ–‡æ³•', 'å¥å‹', 'ç”Ÿæ´»æœƒè©±', 'ä¹éšæ•™æ'],
        )
  langs = st.radio(
        "è«‹é¸æ“‡èªè¨€",
        options=['æ’’å¥‡èŠé›…','é˜¿ç¾','å™¶ç‘ªè˜­','é­¯å‡±','æ’ç£','å‘å—',
                 'è³½å¾·å…‹','é‚µ','æ‹‰é˜¿é­¯å“‡','é”æ‚Ÿ','æ³°é›…','å¤ªé­¯é–£',
                 'é„’','å¡é‚£å¡é‚£å¯Œ','è³½å¤','å¸ƒè¾²'],
                 )
  
  # select a source
  if sources == "è©å…¸":
    s_filt = df['From'] == "è©å…¸"
  elif sources == "æ–‡æ³•":
    s_filt = df['From'] == "æ–‡æ³•"
  elif sources == "å¥å‹":
    s_filt = df['From'] == "å¥å‹"
  elif sources == "ç”Ÿæ´»æœƒè©±":
    s_filt = df['From'] == "ç”Ÿæ´»æœƒè©±"
  elif sources == "ä¹éšæ•™æ":
    s_filt = df['From'] == "ä¹éšæ•™æ"
  
  # select a language 
  if langs == "å™¶ç‘ªè˜­":
    l_filt = df['Lang_En'] == "Kavalan"
  elif langs == "é˜¿ç¾":
    l_filt = df['Lang_En'] == "Amis"
  elif langs == "æ’’å¥‡èŠé›…":
    l_filt = df['Lang_En'] == "Sakizaya"
  elif langs == "é­¯å‡±":
    l_filt = df['Lang_En'] == "Rukai"
  elif langs == "æ’ç£":
    l_filt = df['Lang_En'] == "Paiwan"
  elif langs == "å‘å—":
    l_filt = df['Lang_En'] == "Puyuma"
  elif langs == "è³½å¾·å…‹":
    l_filt = df['Lang_En'] == "Seediq"
  elif langs == "é‚µ":
    l_filt = df['Lang_En'] == "Thao"
  elif langs == "æ‹‰é˜¿é­¯å“‡":
    l_filt = df['Lang_En'] == "Saaroa"
  elif langs == "é”æ‚Ÿ":
    l_filt = df['Lang_En'] == "Yami"
  elif langs == "æ³°é›…":
    l_filt = df['Lang_En'] == "Atayal"
  elif langs == "å¤ªé­¯é–£":
    l_filt = df['Lang_En'] == "Truku"
  elif langs == "é„’":
    l_filt = df['Lang_En'] == "Tsou"
  elif langs == "å¡é‚£å¡é‚£å¯Œ":
    l_filt = df['Lang_En'] == "Kanakanavu"
  elif langs == "è³½å¤":
    l_filt = df['Lang_En'] == "Saisiyat"
  elif langs == "å¸ƒè¾²":
    l_filt = df['Lang_En'] == "Bunun"

  filt_df = df[(s_filt) & (l_filt)]
  zh_columns = {'Lang_Ch': 'èªè¨€_æ–¹è¨€', 'Ab': 'æ—èª', 'Ch': 'è¯èª', 'From': 'ä¾†æº'}
  filt_df.rename(columns=zh_columns, inplace=True)
  st.dataframe(filt_df)

@st.cache
def get_data():
  df = pd.read_pickle('Formosan-Mandarin_sent_pairs_updated.pkl')
  del df['Num']
  clean_df = df.astype('str')
  clean_df = clean_df.apply(strip)  
  return clean_df

if __name__ == '__main__':
  main()
