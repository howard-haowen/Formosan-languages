import pandas as pd
import streamlit as st
import streamlit.components.v1 as components
import re
from pandas_profiling import ProfileReport

def main():
  st.title("å°ç£å—å³¶èª-è¯èªå¥åº«è³‡æ–™é›†")
  st.subheader("Dataset of Formosan-Mandarin sentence pairs")
  st.markdown(
    """
![visitors](https://visitor-badge.glitch.me/badge?page_id=howard-haowen.Formosan-languages)

### è³‡æ–™é›†
- ğŸ¢ è³‡æ–™é›†åˆè¨ˆ`139023`ç­†å¥å°
- â€¼ï¸ æ­¤æŸ¥è©¢ç³»çµ±åƒ…ä¾›æ•™å­¸èˆ‡ç ”ç©¶ä¹‹ç”¨ï¼Œå…§å®¹ç‰ˆæ¬Šæ­¸åŸå§‹è³‡æ–™æä¾›è€…æ‰€æœ‰

### è³‡æ–™ä¾†æº
- ğŸ¥… ä¹éšæ•™æ: [æ—èªEæ¨‚åœ’](http://web.klokah.tw)
- ğŸ’¬ ç”Ÿæ´»æœƒè©±: [æ—èªEæ¨‚åœ’](http://web.klokah.tw)
- ğŸ§— å¥å‹: [æ—èªEæ¨‚åœ’](http://web.klokah.tw)
- ğŸ”­ æ–‡æ³•: [è‡ºç£å—å³¶èªè¨€å¢æ›¸](https://alilin.apc.gov.tw/tw/)
   + ä»¥ä¸Šä¾†æºçš„è³‡æ–™æ˜¯é€éç¶²è·¯çˆ¬èŸ²å–å¾—ã€‚
- ğŸ“š è©å…¸: [åŸä½æ°‘æ—èªè¨€ç·šä¸Šè©å…¸](https://e-dictionary.apc.gov.tw/Index.htm?fbclid=IwAR18XBJPj2xs7nhpPlIUZ-P3joQRGXx22rbVcUvp14ysQu6SdrWYvo7gWCc)
   + è©å…¸è³‡æ–™æ˜¯é€é`PDFMiner` å°‡2019ç‰ˆçš„PDFæª”è½‰æˆHTMLï¼Œå†ç”¨`BeautifulSoup`æŠ“å–å¥å°ï¼Œå¶çˆ¾æœƒå‡ºç¾æ—èªè·Ÿè¯èªå°ä¸ä¸Šçš„æƒ…å½¢ã€‚è‹¥ç™¼ç¾éŒ¯èª¤ï¼Œè«‹[è¯çµ¡æˆ‘ğŸ“©](https://github.com/howard-haowen)ã€‚è©å…¸ä¸­é‡è¤‡å‡ºç¾çš„å¥å­å·²å¾è³‡æ–™é›†ä¸­åˆªé™¤ã€‚

### ä½¿ç”¨æ–¹æ³•
- ğŸ”­ éæ¿¾ï¼šé¸æ“‡ä¾†æºèˆ‡èªè¨€
- ğŸ“š æ’åºï¼šé»é¸é¦–æ¬„
- ğŸ¥… æ”¾å¤§ï¼šé»é¸è¡¨æ ¼å³ä¸Šè§’é€²å…¥å…¨è¢å¹•æ¨¡å¼
- ğŸ’¬ æ›´å¤šï¼šå¥å­å¤ªé•·æ™‚ï¼Œå°‡æ»‘é¼ ç§»åˆ°å¥å­ä¸Šæ–¹å³å¯æª¢è¦–å®Œæ•´å…§å®¹
"""
)
  # fetch the raw data
  df = get_data()
  # pd.set_option('max_colwidth', 600)
  
  # remap column names
  zh_columns = {'Lang_En': 'Language','Lang_Ch': 'èªè¨€_æ–¹è¨€', 'Ab': 'æ—èª', 'Ch': 'è¯èª', 'From': 'ä¾†æº'}
  df.rename(columns=zh_columns, inplace=True)
  
  # set up filtering options
  source_set = df['ä¾†æº'].unique()
  sources = st.sidebar.multiselect(
        "è«‹é¸æ“‡ä¾†æº",
        options=source_set,
        default='è©å…¸',)
  langs = st.sidebar.selectbox(
        "è«‹é¸æ“‡èªè¨€",
        options=['å¸ƒè¾²','é˜¿ç¾','æ’’å¥‡èŠé›…','å™¶ç‘ªè˜­','é­¯å‡±','æ’ç£','å‘å—',
                 'æ³°é›…','è³½å¾·å…‹','å¤ªé­¯é–£','é„’','æ‹‰é˜¿é­¯å“‡','å¡é‚£å¡é‚£å¯Œ',
                 'é‚µ','è³½å¤','é”æ‚Ÿ'],)
  texts = st.sidebar.radio(
        "è«‹é¸æ“‡æŸ¥è©¢æ–‡å­—é¡åˆ¥",
        options=['è¯èª','æ—èª'],)
    
  # filter by sources
  s_filt = df['ä¾†æº'] in sources
  
  # select a language 
  if langs == "å™¶ç‘ªè˜­":
    l_filt = df['Language'] == "Kavalan"
  elif langs == "é˜¿ç¾":
    l_filt = df['Language'] == "Amis"
  elif langs == "æ’’å¥‡èŠé›…":
    l_filt = df['Language'] == "Sakizaya"
  elif langs == "é­¯å‡±":
    l_filt = df['Language'] == "Rukai"
  elif langs == "æ’ç£":
    l_filt = df['Language'] == "Paiwan"
  elif langs == "å‘å—":
    l_filt = df['Language'] == "Puyuma"
  elif langs == "è³½å¾·å…‹":
    l_filt = df['Language'] == "Seediq"
  elif langs == "é‚µ":
    l_filt = df['Language'] == "Thao"
  elif langs == "æ‹‰é˜¿é­¯å“‡":
    l_filt = df['Language'] == "Saaroa"
  elif langs == "é”æ‚Ÿ":
    l_filt = df['Language'] == "Yami"
  elif langs == "æ³°é›…":
    l_filt = df['Language'] == "Atayal"
  elif langs == "å¤ªé­¯é–£":
    l_filt = df['Language'] == "Truku"
  elif langs == "é„’":
    l_filt = df['Language'] == "Tsou"
  elif langs == "å¡é‚£å¡é‚£å¯Œ":
    l_filt = df['Language'] == "Kanakanavu"
  elif langs == "è³½å¤":
    l_filt = df['Language'] == "Saisiyat"
  elif langs == "å¸ƒè¾²":
    l_filt = df['Language'] == "Bunun"
  
  # create a text box for keyword search
  text_box = st.text_input('é—œéµè©æŸ¥è©¢ï¼šåœ¨æ­¤è¼¸å…¥è¯èªæˆ–æ—èªï¼ŒæŒ‰ä¸‹ENTERå¾Œæœƒè‡ªå‹•æ›´æ–°æŸ¥è©¢çµæœã€‚')
  st.markdown(
    """
- ğŸ” å­—ä¸²æŸ¥è©¢æ”¯æ´[æ­£å‰‡è¡¨é”å¼](https://zh.wikipedia.org/zh-tw/æ­£åˆ™è¡¨è¾¾å¼)
- ğŸ¥³ æ—èªç¯„ä¾‹: ä½¿ç”¨`cia *`æŸ¥è©¢å¸ƒè¾²èªï¼Œèƒ½æ‰¾åˆ°åŒ…å«`danumcia`ã€`luduncia`æˆ–`siulcia`ç­‰è©çš„å¥å­
- ğŸ¤© è¯èªç¯„ä¾‹: ä½¿ç”¨`^æœ‰ä¸€`æŸ¥è©¢è¯èªï¼Œèƒ½æ‰¾åˆ°ä½¿ç”¨`æœ‰ä¸€å¤©`ã€`æœ‰ä¸€å¡Š`æˆ–`æœ‰ä¸€æ™š`ç­‰è©å‡ºç¾åœ¨å¥é¦–çš„å¥å­
"""
)
  # search for keywords in Mandarin or Formosan 
  t_filt = df[texts].str.contains(text_box, flags=re.IGNORECASE)
  
  # filter the data based on all criteria
  filt_df = df[(s_filt)&(l_filt)&(t_filt)]
  
  st.markdown(
    """
### æŸ¥è©¢çµæœ
"""
)
  # display the filtered data
  st.dataframe(filt_df)
 
  st.markdown(
    """
### è³‡æ–™é›†çµ±è¨ˆçµæœ
"""
)
  # display a data profile report
  report = ProfileReport(df, title='è³‡æ–™é›†çµ±è¨ˆ', minimal=True).to_html()
  components.html(report, width=800, height=1200, scrolling=True)  
  
# Cache the raw data to speed up subseuqent requests 
@st.cache
def get_data():
  df = pd.read_pickle('Formosan-Mandarin_sent_pairs_139023entries.pkl')
  df = df.astype('str')
  df = df.applymap(lambda x: x.strip() if isinstance(x, str) else x)
  filt = df.Ch == "-"
  df = df[~filt] # remove ungrammatical sentences in the grammar book of Katripul Puyuma 

  return df

if __name__ == '__main__':
  main()
